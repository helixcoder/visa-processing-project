# -*- coding: utf-8 -*-
"""visa-project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TlUk74Q_fMhn0U9M8MyHcHehCXHLDndF
"""

import pandas as pd
import numpy as np
import os

INPUT_PATHS = [
    "/mnt/data/us_perm_visas.csv",
    "/content/us_perm_visas.csv",
    "us_perm_visas.csv"
]
OUTPUT_PATH = "/mnt/data/us_perm_visas_preprocessed.csv"


INPUT_PATH = next((p for p in INPUT_PATHS if os.path.exists(p)), None)
print("Selected INPUT_PATH =", INPUT_PATH)
print("OUTPUT_PATH =", OUTPUT_PATH)

if INPUT_PATH:
    df = pd.read_csv(INPUT_PATH, low_memory=False)
    print(f"Loaded file: {INPUT_PATH} -> shape: {df.shape}")
else:
    print("Input file not found. Creating a synthetic sample for demonstration.")
    df = pd.DataFrame({
        "case_id":[1,2,3,4],
        "received_date":["2019-10-01","2019-11-05","2020-01-15","2020-02-10"],
        "decision_date":["2020-02-01","2020-01-20","2020-02-20", None],
        "employer_name":["A Corp","B LLC","C Inc","D Ltd"],
        "job_title":["SWE","Data Scientist","Analyst","SWE"],
        "wage":[120000, 110000, np.nan, 90000],
        "case_status":["Certified","Denied","Certified","Certified"],
        "country_of_birth":["India","China","India","USA"]
    })
    print("Synthetic sample shape:", df.shape)

display(df.head())
print("Columns:", df.columns.tolist())

def detect_date_cols(df):
    parse_counts = {}
    for col in df.columns:
        parsed = pd.to_datetime(df[col], errors='coerce', infer_datetime_format=True)
        parse_counts[col] = int(parsed.notna().sum())
    return sorted(parse_counts.items(), key=lambda x: -x[1])

date_candidates = detect_date_cols(df)
print("Top date-like columns (col, parseable_count):")
print(date_candidates[:8])

def pick_start_end(date_candidates):
    cols = [c for c,_ in date_candidates if _>0]
    start = None; end = None
    start_keywords = ['received','receive','file','submitted','start','application','submit']
    end_keywords = ['decision','approve','approval','certified','complete','completed','end','adjudicated']
    for c in cols:
        lc = c.lower()
        if any(k in lc for k in start_keywords) and start is None:
            start = c
        if any(k in lc for k in end_keywords) and end is None:
            end = c
    if (start is None or end is None) and len(cols) >= 2:
        start, end = cols[0], cols[1]
    if start and end and start == end:
        end = None
    return start, end

start_col, end_col = pick_start_end(date_candidates)
print("Chosen start column:", start_col)
print("Chosen end   column:", end_col)

if start_col:
    df['_start_dt'] = pd.to_datetime(df[start_col], errors='coerce', infer_datetime_format=True)
if end_col:
    df['_end_dt'] = pd.to_datetime(df[end_col], errors='coerce', infer_datetime_format=True)

if start_col and end_col:
    df['processing_time_days'] = (df['_end_dt'] - df['_start_dt']).dt.days
    df.loc[df['processing_time_days'] < 0, 'processing_time_days'] = np.nan
    df.loc[df['processing_time_days'] > 3650, 'processing_time_days'] = np.nan
else:
    df['processing_time_days'] = np.nan

print("processing_time_days sample:")
print(df[['processing_time_days', start_col, end_col]].head(10).to_string())

print("Missing counts (top):")
print(df.isna().sum().sort_values(ascending=False).head(20))

missing_target = df['processing_time_days'].isna().sum()
print(f"\nRows with missing target 'processing_time_days': {missing_target}")

df_model = df.dropna(subset=['processing_time_days']).copy()
print("After dropping rows without target:", df_model.shape)

numeric_cols = df_model.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols = [c for c in numeric_cols if c != 'processing_time_days']
for c in numeric_cols:
    med = df_model[c].median()
    df_model[c] = df_model[c].fillna(med)

cat_cols = df_model.select_dtypes(include=['object','string']).columns.tolist()
for c in cat_cols:
    df_model[c] = df_model[c].fillna('Unknown')

print("\nAfter basic imputation, sample:")
display(df_model.head())

def encode_categoricals_fast(df, onehot_thresh=8, max_total_onehot_cols=1000):
    df2 = df.copy()
    cat_cols = df2.select_dtypes(include=['object','string']).columns.tolist()
    onehot_cols = []
    freq_cols = []
    total_onehot_new_cols = 0
    for col in cat_cols:
        n_unique = df2[col].nunique(dropna=False)
        if n_unique <= onehot_thresh:
            total_onehot_new_cols += n_unique
            if total_onehot_new_cols <= max_total_onehot_cols:
                onehot_cols.append(col)
            else:
                freq_cols.append(col)
        else:
            freq_cols.append(col)

    if onehot_cols:
        dummies = pd.get_dummies(df2[onehot_cols].astype(str), prefix=onehot_cols, dummy_na=False)
        df2 = pd.concat([df2.drop(columns=onehot_cols), dummies], axis=1)

    for col in freq_cols:
        freq = df2[col].value_counts(normalize=True)
        df2[col + "_freq_enc"] = df2[col].map(freq).fillna(0.0)
        df2 = df2.drop(columns=[col])

    return df2

df_preprocessed = encode_categoricals_fast(df_model, onehot_thresh=8, max_total_onehot_cols=1000)
print("Preprocessed shape:", df_preprocessed.shape)
print("Columns (first 40):", df_preprocessed.columns.tolist()[:40])
display(df_preprocessed.head())

out_dir = os.path.dirname(OUTPUT_PATH)
if out_dir and not os.path.exists(out_dir):
    try:
        os.makedirs(out_dir)
    except Exception:
        pass

df_preprocessed.to_csv(OUTPUT_PATH, index=False)
print("Saved preprocessed file to:", OUTPUT_PATH)

if 'processing_time_days' in df_preprocessed.columns:
    print("\nprocessing_time_days summary:")
    print(df_preprocessed['processing_time_days'].describe())
else:
    print("processing_time_days not present in preprocessed dataframe.")

from google.colab import files

files.download(OUTPUT_PATH)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use("default")
sns.set_context("notebook")

DATA_PATH = "/content/us_perm_visas_preprocessed.csv"

df = pd.read_csv(DATA_PATH)
print("Dataset shape:", df.shape)

df = df[df['processing_time_days'].notna()]
df.head()

plt.figure(figsize=(8,5))
sns.histplot(df['processing_time_days'], bins=50, kde=True)
plt.title("Distribution of Visa Processing Time (Days)")
plt.xlabel("Processing Time (Days)")
plt.ylabel("Frequency")
plt.show()

visa_col = next((c for c in df.columns if 'visa' in c.lower() or 'class' in c.lower()), None)

if visa_col:
    plt.figure(figsize=(10,5))
    sns.boxplot(data=df, x=visa_col, y='processing_time_days')
    plt.xticks(rotation=45)
    plt.title("Processing Time by Visa Type")
    plt.show()
else:
    print("Visa type column not found")

region_col = next((c for c in df.columns if 'country' in c.lower()), None)

if region_col:
    top_regions = df[region_col].value_counts().head(10).index
    region_df = df[df[region_col].isin(top_regions)]

    plt.figure(figsize=(10,5))
    sns.boxplot(data=region_df, x=region_col, y='processing_time_days')
    plt.xticks(rotation=45)
    plt.title("Processing Time by Applicant Origin (Top 10 Countries)")
    plt.show()
else:
    print("Region column not found")

date_col = next((c for c in df.columns if 'date' in c.lower()), None)

if date_col:
    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
    df['month'] = df[date_col].dt.month

    plt.figure(figsize=(8,5))
    sns.lineplot(data=df, x='month', y='processing_time_days')
    plt.title("Seasonal Trend in Visa Processing Time")
    plt.xlabel("Month")
    plt.ylabel("Average Processing Time (Days)")
    plt.show()
else:
    print("Date column not found")

center_col = next((c for c in df.columns if 'center' in c.lower()), None)

if center_col:
    workload = df.groupby(center_col)['processing_time_days'].mean().sort_values(ascending=False).head(10)

    workload.plot(kind='bar', figsize=(10,5))
    plt.title("Average Processing Time by Processing Center")
    plt.ylabel("Days")
    plt.show()
else:
    print("Processing center column not found")

numeric_df = df.select_dtypes(include=['int64','float64'])

plt.figure(figsize=(10,6))
sns.heatmap(numeric_df.corr(), cmap="coolwarm", center=0)
plt.title("Feature Correlation Heatmap")
plt.show()

from sklearn.ensemble import RandomForestRegressor

X = numeric_df.drop(columns=['processing_time_days'], errors='ignore')
y = df['processing_time_days']

rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
rf.fit(X, y)

importance = pd.Series(rf.feature_importances_, index=X.columns)
importance = importance.sort_values(ascending=False).head(10)

plt.figure(figsize=(8,5))
importance.plot(kind='bar')
plt.title("Top 10 Feature Importances")
plt.ylabel("Importance Score")
plt.show()